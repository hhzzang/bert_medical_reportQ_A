{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRp9nuLKmzQF","executionInfo":{"status":"ok","timestamp":1613438073357,"user_tz":-540,"elapsed":15349,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"9858c54f-1e17-4601-a876-94a66f8be592"},"source":["# mount google drive \n","\n","import os, sys \n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"580iH29b-K9y","executionInfo":{"status":"ok","timestamp":1613370635157,"user_tz":-540,"elapsed":602,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"91b00ef0-5b70-4982-8aec-83340602013b"},"source":["import os\r\n","print(os.getcwd())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sDNNPxBMN0Vo","executionInfo":{"status":"ok","timestamp":1613438087323,"user_tz":-540,"elapsed":16612,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"0e362c93-9e6d-4dbe-f8e6-eb2ebed14d8d"},"source":["!pip install transformers\r\n","!pip install tokenizers\r\n","!pip install tensorflow_addons"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 5.7MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 34.4MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 31.6MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=835fd9b01eeab3118d75e91aa6e51ca6fb249cb90b00651de60c83d289c67d4e\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.10.1)\n","Collecting tensorflow_addons\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/af/0ce633c373d2b0476ef8299673d22275fcc3c5ba283b2cec4aa06bc5b810/tensorflow_addons-0.12.1-cp36-cp36m-manylinux2010_x86_64.whl (703kB)\n","\u001b[K     |████████████████████████████████| 706kB 4.2MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.12.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_PkU4-jFVx6-","executionInfo":{"status":"ok","timestamp":1613438117056,"user_tz":-540,"elapsed":4999,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"60c43b71-fb79-42bb-f57d-55f4c2f6302f"},"source":["# import and necessary fuctions for prediction\r\n","import numpy as np\r\n","import re\r\n","import string\r\n","import json\r\n","import tensorflow as tf\r\n","import tensorflow_addons as tfa\r\n","import keras\r\n","from tokenizers import BertWordPieceTokenizer\r\n","from transformers import BertTokenizer, TFBertModel\r\n","MAX_LEN = 511\r\n","VERBOSE = 2\r\n","BATCH_SIZE = 8\r\n","tokenizer = BertWordPieceTokenizer(\"/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src/bert-base-multilingual-cased/vocab.txt\", lowercase=False)\r\n","\r\n","def normalized_answer(s):\r\n","    def remove_(text):\r\n","        ''' 불필요한 기호 제거 '''\r\n","        text = re.sub(\"'\", \" \", text)\r\n","        text = re.sub('\"', \" \", text)\r\n","        text = re.sub('《', \" \", text)\r\n","        text = re.sub('》', \" \", text)\r\n","        text = re.sub('<', \" \", text)\r\n","        text = re.sub('>', \" \", text)\r\n","        text = re.sub('〈', \" \", text)\r\n","        text = re.sub('〉', \" \", text)\r\n","        text = re.sub(\"\\(\", \" \", text)\r\n","        text = re.sub(\"\\)\", \" \", text)\r\n","        text = re.sub(\"‘\", \" \", text)\r\n","        text = re.sub(\"’\", \" \", text)\r\n","        return text\r\n","\r\n","    def white_space_fix(text):\r\n","        return ' '.join(text.split())\r\n","\r\n","    def remove_punc(text):\r\n","        exclude = set(string.punctuation)\r\n","        return ''.join(ch for ch in text if ch not in exclude)\r\n","\r\n","    def lower(text):\r\n","        return text.lower()\r\n","\r\n","    return white_space_fix(remove_punc(lower(remove_(s))))\r\n","\r\n","class TFBERTQuestionAnswering(tf.keras.Model):\r\n","    def __init__(self, model_name, dir_path, num_class):\r\n","        super(TFBERTQuestionAnswering, self).__init__()\r\n","        \r\n","        self.encoder = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\r\n","        self.start_logit = tf.keras.layers.Dense(num_class, name=\"start_logit\", use_bias=False)\r\n","        self.end_logit = tf.keras.layers.Dense(num_class, name=\"end_logit\", use_bias=False)\r\n","        self.flatten = tf.keras.layers.Flatten() \r\n","        self.softmax = tf.keras.layers.Activation(tf.keras.activations.softmax)\r\n","        \r\n","    def call(self, inputs):\r\n","        input_ids, token_type_ids, attention_mask = inputs\r\n","        embedding = self.encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\r\n","        start_logits = self.start_logit(embedding)\r\n","        start_logits = self.flatten(start_logits)\r\n","        \r\n","        end_logits = self.end_logit(embedding)\r\n","        end_logits = self.flatten(end_logits)\r\n","        \r\n","        start_probs = self.softmax(start_logits)\r\n","        end_probs = self.softmax(end_logits)\r\n","    \r\n","        return start_probs, end_probs\r\n","\r\n","class SquadExample:\r\n","    def __init__(self, question, context, start_char_idx, answer_text):\r\n","        self.question = question\r\n","        self.context = context\r\n","        self.start_char_idx = start_char_idx\r\n","        self.answer_text = answer_text\r\n","        self.skip = False\r\n","\r\n","    def preprocess(self):\r\n","        context = self.context\r\n","        question = self.question\r\n","        answer_text = self.answer_text\r\n","        start_char_idx = self.start_char_idx\r\n","\r\n","        # Clean context, answer and question\r\n","        context = \" \".join(str(context).split())\r\n","        question = \" \".join(str(question).split())\r\n","        answer = \" \".join(str(answer_text).split())\r\n","\r\n","        # Find end character index of answer in context\r\n","        end_char_idx = start_char_idx + len(answer)\r\n","        if end_char_idx >= len(context):\r\n","            self.skip = True\r\n","            return\r\n","\r\n","        # Mark the character indexes in context that are in answer\r\n","        is_char_in_ans = [0] * len(context)\r\n","        for idx in range(start_char_idx, end_char_idx):\r\n","            is_char_in_ans[idx] = 1\r\n","\r\n","        # Tokenize context\r\n","        tokenized_context = tokenizer.encode(context)\r\n","\r\n","        # Find tokens that were created from answer characters\r\n","        ans_token_idx = []\r\n","        for idx, (start, end) in enumerate(tokenized_context.offsets):\r\n","            if sum(is_char_in_ans[start:end]) > 0:\r\n","                ans_token_idx.append(idx)\r\n","\r\n","        if len(ans_token_idx) == 0:\r\n","            self.skip = True\r\n","            return\r\n","\r\n","        # Find start and end token index for tokens from answer\r\n","        start_token_idx = ans_token_idx[0]\r\n","        end_token_idx = ans_token_idx[-1]\r\n","\r\n","        # Tokenize question\r\n","        tokenized_question = tokenizer.encode(question)\r\n","\r\n","        # Create inputs\r\n","        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\r\n","        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\r\n","            tokenized_question.ids[1:]\r\n","        )\r\n","        attention_mask = [1] * len(input_ids)\r\n","\r\n","        # Pad and create attention masks.\r\n","        # Skip if truncation is needed\r\n","        padding_length = MAX_LEN - len(input_ids)\r\n","        if padding_length > 0:  # pad\r\n","            input_ids = input_ids + ([0] * padding_length)\r\n","            attention_mask = attention_mask + ([0] * padding_length)\r\n","            token_type_ids = token_type_ids + ([0] * padding_length)\r\n","        elif padding_length < 0:  # skip\r\n","            self.skip = True\r\n","            return\r\n","        \r\n","        #함정카드\r\n","        # Truncate the comments\r\n","        if len(input_ids) >  512:\r\n","            input_ids = input_ids[0:128] + input_ids[-382:-1]\r\n","            attention_mask = attention_mask[0:128] + attention_mask[-382:-1]\r\n","            token_type_ids = token_type_ids[0:128] + token_type_ids[-382:-1]\r\n","        #####\r\n","        \r\n","        self.input_ids = input_ids\r\n","        self.token_type_ids = token_type_ids\r\n","        self.attention_mask = attention_mask\r\n","        self.start_token_idx = start_token_idx\r\n","        self.end_token_idx = end_token_idx\r\n","        self.context_token_to_char = tokenized_context.offsets\r\n","\r\n","\r\n","def create_squad_examples(raw_data):\r\n","    squad_examples = []\r\n","    for item in raw_data[\"data\"]:\r\n","        for para in item[\"paragraphs\"]:\r\n","            context = para[\"context\"]\r\n","            for qa in para[\"qas\"]:\r\n","                question = qa[\"question\"]\r\n","                answer_text = qa[\"answers\"][0][\"text\"]\r\n","                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\r\n","                squad_eg = SquadExample(\r\n","                    question, context, start_char_idx, answer_text\r\n","                )\r\n","                squad_eg.preprocess()\r\n","                squad_examples.append(squad_eg)\r\n","    return squad_examples\r\n","\r\n","\r\n","def create_inputs_targets(squad_examples):\r\n","    dataset_dict = {\r\n","        \"input_ids\": [],\r\n","        \"token_type_ids\": [],\r\n","        \"attention_mask\": [],\r\n","        \"start_token_idx\": [],\r\n","        \"end_token_idx\": [],\r\n","    }\r\n","    for item in squad_examples:\r\n","        #item = suqand_example class\r\n","        if item.skip == False:\r\n","            for key in dataset_dict:\r\n","                dataset_dict[key].append(getattr(item, key))\r\n","    for key in dataset_dict:\r\n","        dataset_dict[key] = np.array(dataset_dict[key])\r\n","\r\n","    x = [\r\n","        dataset_dict[\"input_ids\"],\r\n","        dataset_dict[\"token_type_ids\"],\r\n","        dataset_dict[\"attention_mask\"],\r\n","    ]\r\n","    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\r\n","    return x, y\r\n","\r\n","path = '/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src/'\r\n","with open(path+'extract_train_data.json') as f:\r\n","    raw_train_data = json.load(f)\r\n","\r\n","with open(path+'extract_dev_data.json') as f:\r\n","    raw_eval_data = json.load(f)\r\n","\r\n","\r\n","train_squad_examples = create_squad_examples(raw_train_data)\r\n","x_train, y_train = create_inputs_targets(train_squad_examples)\r\n","print(f\"{len(train_squad_examples)} training points created.\")\r\n","\r\n","eval_squad_examples = create_squad_examples(raw_eval_data)\r\n","x_eval, y_eval = create_inputs_targets(eval_squad_examples)\r\n","print(f\"{len(eval_squad_examples)} evaluation points created.\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["3487 training points created.\n","435 evaluation points created.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"isVVOhusOmHi","executionInfo":{"status":"ok","timestamp":1613370334178,"user_tz":-540,"elapsed":70610,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"49e35080-552b-420b-8c50-ab4af0080c16"},"source":["##model load\r\n","\r\n","\r\n","\r\n","loaded_model = TFBERTQuestionAnswering(model_name='/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src/bert-base-multilingual-cased/',dir_path='./src/bert_ckpt', num_class=1)\r\n","optimizer = tfa.optimizers.RectifiedAdam(lr=1e-5) #tf.keras.optimizers.RAdam(learning_rate=5e-5)\r\n","loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\r\n","loaded_model.compile(optimizer=optimizer, loss=[loss, loss])\r\n","for_batch = [np.array([x_train[0][0]]),np.array([x_train[1][0]]),np.array([x_train[2][0]])]\r\n","for_batch_y = [np.array([y_train[0][0]]),np.array([y_train[1][0]])]\r\n","history = loaded_model.fit(\r\n","    for_batch,\r\n","    for_batch_y,\r\n","    epochs=1, \r\n","    verbose=VERBOSE,\r\n","    batch_size=16,\r\n","\r\n",")\r\n","\r\n","loaded_model.load_weights('/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src/6_epochs_extract_radam_weights.h5')\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some layers from the model checkpoint at /content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src/bert-base-multilingual-cased/ were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at /content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src/bert-base-multilingual-cased/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss.\n","1/1 - 56s - loss: 13.7386 - output_1_loss: 7.2176 - output_2_loss: 6.5210\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IhReelv6qB8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613376449921,"user_tz":-540,"elapsed":5781263,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"d7cea285-c58e-4063-8a76-322194ba906b"},"source":["# load dev_input.txt and make dev_result.txt file which is prediction\n","with open('/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src/question_ex.json') as f:\n","    question_ex = json.load(f)\n","\n","eval_data = []\n","path = '/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/'\n","with open(path+'data/dev/dev_input.txt') as f:\n","    for i in f:\n","        temp = i.split('\\t')\n","        context = temp[0].strip()\n","        question = temp[1].strip()\n","        make_data = {'paragraphs':[{'qas':[{'answers':[{'text':None,'answer_start': None,'id':'for inference'}],'question': question}],'context': context}]}\n","        eval_data.append(make_data)\n","\n","eval_result = {}\n","eval_result['version'] = 'for_inference'\n","eval_result['data'] = eval_data\n","\n","\n","\n","class SquadExample2:\n","    def __init__(self, question, context, max_len=511):\n","        self.question = question\n","        self.context = context\n","        self.skip = False\n","        self.max_len = max_len\n","\n","    def preprocess(self):\n","        context = self.context\n","        question = self.question\n","\n","        # Clean context, answer and question\n","        context = \" \".join(str(context).split())\n","        question = \" \".join(str(question).split())\n","\n","        # Tokenize context\n","        tokenized_context = tokenizer.encode(context)\n","\n","        # Tokenize question\n","        tokenized_question = tokenizer.encode(question)\n","\n","        # Create inputs\n","        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n","        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n","            tokenized_question.ids[1:]\n","        )\n","        attention_mask = [1] * len(input_ids)\n","\n","        # Pad and create attention masks.\n","        # Skip if truncation is needed\n","        padding_length = self.max_len - len(input_ids)\n","        if padding_length > 0:  # pad\n","            input_ids = input_ids + ([0] * padding_length)\n","            attention_mask = attention_mask + ([0] * padding_length)\n","            token_type_ids = token_type_ids + ([0] * padding_length)\n","        elif padding_length < 0:  # skip\n","            self.skip = True\n","            return\n","        \n","        #error handling\n","        # Truncate the comments\n","        # if len(input_ids) >  510:\n","        #     input_ids = input_ids[0:128] + input_ids[-382:-1] #510\n","        #     attention_mask = attention_mask[0:128] + attention_mask[-382:-1]\n","        #     token_type_ids = token_type_ids[0:128] + token_type_ids[-382:-1]\n","        ##### \n","        \n","        self.input_ids = input_ids\n","        self.token_type_ids = token_type_ids\n","        self.attention_mask = attention_mask\n","        self.context_token_to_char = tokenized_context.offsets\n","\n","\n","def create_squad_examples2(raw_data):\n","    squad_examples = []\n","    for item in evres:\n","        context = item[\"context\"][0]\n","        question = item[\"question\"]     \n","        squad_eg = SquadExample2(question, context)\n","        squad_eg.preprocess()\n","        squad_examples.append(squad_eg)\n","    return squad_examples\n","\n","\n","def create_inputs_targets2(squad_examples):\n","    dataset_dict = {\n","        \"input_ids\": [],\n","        \"token_type_ids\": [],\n","        \"attention_mask\": []}\n","    \n","    for item in squad_examples:\n","        #item = suqand_example class\n","        if item.skip == False:\n","            for key in dataset_dict:\n","                dataset_dict[key].append(getattr(item, key)) #squad_example.input_ids \n","    for key in dataset_dict:\n","        dataset_dict[key] = np.array(dataset_dict[key])\n","\n","    x = [\n","        dataset_dict[\"input_ids\"],\n","        dataset_dict[\"token_type_ids\"],\n","        dataset_dict[\"attention_mask\"],\n","    ]\n","    \n","    return x\n","\n","max_len = 511\n","inferences = {}\n","for n in range(len(eval_result['data'])):\n","    evres = []\n","    question = eval_result['data'][n]['paragraphs'][0]['qas'][0]['question']\n","    eval_splits = eval_result['data'][n]['paragraphs'][0]['context'].split('.')\n","    \n","    total_txt = []\n","    temp_txt = []\n","    permit = 490\n","\n","    for idx in range(len(eval_splits)):\n","        permit -= len(eval_splits[idx])\n","        if permit < 0:\n","            permit = 490\n","            total_txt.append([\".\".join(temp_txt) + '.'])\n","            temp_txt = []\n","        \n","        temp_txt.append(eval_splits[idx])\n","        if idx == (len(eval_splits) - 1):\n","            total_txt.append([\".\".join(temp_txt)])\n","    evdict = {}\n","\n","    for ids,str_txt in enumerate(total_txt):\n","        # if ids == 0:\n","        evdict['question'] = question\n","        evdict['context'] = str_txt\n","        # else:\n","        #     evdict['context'] = str_txt\n","        #     evdict['question'] = 'below'\n","        evres.append(evdict)\n","        evdict = {}\n","\n","    evres_squad = create_squad_examples2(evres)\n","    x_eval = create_inputs_targets2(evres_squad)\n","    print(f\"{len(evres_squad)} evaluation points created.it is from \",n)\n","    # print(\"답 : \",normalized_answer(raw_eval_data['data'][n]['paragraphs'][0]['qas'][0]['answers'][0]['text']), \"question: \",question)\n","    pred_start, pred_end = loaded_model.predict(x_eval)\n","\n","    eval_examples_no_skip = [_ for _ in evres_squad if _.skip == False] # evres_squad or eval_squad_examples\n","    inference = []\n","    status = 0\n","    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n","        squad_eg = eval_examples_no_skip[idx]\n","        offsets = squad_eg.context_token_to_char\n","\n","        start = np.argmax(start)\n","\n","        end = np.argmax(end)\n","        if start >= len(offsets):\n","            continue\n","        pred_char_start = offsets[start][0]\n","        if end < len(offsets):\n","            pred_char_end = offsets[end][1]\n","            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n","        else:\n","            pred_ans = squad_eg.context[pred_char_start:]\n","        normalized_pred_ans = normalized_answer(pred_ans)\n","        for qs_ex in question_ex[question]:\n","            if normalized_answer(qs_ex.strip()) != '':\n","                if (normalized_answer(qs_ex.strip()) in normalized_pred_ans) or (normalized_pred_ans in normalized_answer(qs_ex.strip())):\n","                    if normalized_pred_ans != '':\n","                        inference.append(normalized_pred_ans)\n","                        status = 1\n","                else:\n","                    if (normalized_pred_ans[:int(len(normalized_pred_ans) / 3)] in normalized_answer(qs_ex.strip())) or (normalized_pred_ans[1 * int(len(normalized_pred_ans) / 4):] in normalized_answer(qs_ex.strip())):\n","                        if normalized_pred_ans != '':\n","                            inference.append(normalized_pred_ans)\n","                            status = 1\n","        inference = np.unique(inference).tolist()\n","    if status == 1:\n","        if (inference != ['']):\n","            inferences[n] = inference\n","    else:\n","        temp_li = []\n","        for qex in question_ex[question]:\n","            if normalized_pred_ans[:5] in qex:\n","                temp_li.append(qex)\n","        len_max = len(temp_li)\n","        if len_max == 0:\n","            inferences[n] = [normalized_answer(question_ex[question][np.random.randint(0,len(question_ex[question]))])]\n","        else:\n","            inferences[n] = [normalized_answer(temp_li[np.random.randint(0,len_max)])]\n","#inferences\n","ban_list = ['a 환자분 입원하러 오셨나요 b 네','a 입원하러 오셨나요 b 네 오후 4시에 병동으로 올라오라고 했어요','a 네 되셨습니다','a 안녕하세요 혹시 오늘 어떤 것 때문에 병원에 오시게 되셨을까요 b 아 네 안녕하세요 다름이 아니라 대변에서 피가나오는 증상이랑 설사 증상이 있어서 방문하게 되었습니다 b 일주일 전쯤에 상한 음식을 먹고 계속 그러네요','a 안녕하세요 혹시 오늘 어떤 것 때문에 병원에 오시게 되셨을까요 b 숨쉬기가 힘들어요 a 언제부터 그러셨나요 b 몇 일 전부터 기침이 계속 나오더니 삼십분전부터 숨쉬기가 힘들어요','a 안녕하세요 혹시 오늘 어떤 것 때문에 병원에 오시게 되셨을까요 b 숨쉬기가 힘들어요','a 기형이 있으신가요 b 아니요 없습니다','a 흡연을 하시나요 b 네 그렇습니다','a 발진이 있으신가요 b 아니요 없습니다','a 흡연을 하시나요 b 네 그렇습니다 a 흡연량이 얼마나 되나요 b 일주일에 한 갑 피웁니다 a 흡연기간이 얼마나 되시나요 b 30년 정도 되었습니다 a 음주를 하시나요 b 네 그렇습니다 a 음주량과 횟수 종류가 어떻게 되시나요 b 일주일에 23번 마시고 한 번에 소주 2병 정도 마십니다 a 음주기간이 얼마나 되시나요 b 30년 조금 넘었습니다 a 어디를 통해서 입원을 하셨나요 b 이 병원 외래를 방문 후 입원하게 되었습니다','a 최근 삼 개월 이내에 입원치료를 받으신 적이 있나요 b 아니요 없습니다 a 흡연을 하시나요 b 네 그렇습니다 a 흡연량이 얼마나 되나요 b 일주일에 한 갑 피웁니다 a 흡연기간이 얼마나 되시나요 b 30년 정도 되었습니다 a 음주를 하시나요 b 네 그렇습니다 a 음주량과 횟수 종류가 어떻게 되시나요 b 일주일에 23번 마시고 한 번에 소주 2병 정도 마십니다 a 음주기간이 얼마나 되시나요 b 30년 조금 넘었습니다 a 어디를 통해서 입원을 하셨나요 b 이 병원 외래를 방문 후 입원하게 되었습니다','a 음식이나 약에 알레르기 반응이 있으신가요 b 아니요 없습니다','a 수술이나 입원치료를 받으신 적이 있으신가요 b 아니요 없습니다','a 수혈을 하신 적이 있으신가요 b 아니요 없습니다','a 현재 통증이 있으신가요 b 네 있습니다','a 최근 삼 개월 이내에 입원치료를 받으신 적이 있나요 b 아니요 없습니다 a 흡연을 하시나요 b 네 그렇습니다 a 흡연량이 얼마나 되나요 b 일주일에 한 갑 피웁니다 a 흡연기간이 얼마나 되시나요 b 30년 정도 되었습니다 a 음주를 하시나요 b 네 그렇습니다 a 음주량과 횟수 종류가 어떻게 되시나요 b 일주일에 23번 마시고 한 번에 소주 2병 정도 마십니다 a 음주기간이 얼마나 되시나요 b 30년 조금 넘었습니다 a 어디를 통해서 입원을 하셨나요 b 이 병원 외래를 방문 후 입원하게 되었습니다']\n","for i in inferences.keys():\n","    if len(inferences[i]) > 1:\n","        not_ban = []\n","        for inf in inferences[i]:\n","            if inf not in ban_list:\n","                not_ban.append(inf)\n","        inferences[i] = not_ban\n","        \n","        if len(inferences[i]) > 1:\n","            inferences[i] = [inferences[i][0]]\n","            \n","with open('/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/result/dev_result.txt','w',encoding='utf-8') as f:\n","    for ke in inferences.keys():\n","        if inferences[ke] == []:\n","            f.write('A: ~ 인가요? B: 네. 아니요')\n","            f.write('\\n')\n","        else:\n","            f.write(inferences[ke][0])\n","            f.write('\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4 evaluation points created.it is from  0\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","4 evaluation points created.it is from  1\n","4 evaluation points created.it is from  2\n","this is random\n","4 evaluation points created.it is from  3\n","4 evaluation points created.it is from  4\n","this is random\n","4 evaluation points created.it is from  5\n","4 evaluation points created.it is from  6\n","4 evaluation points created.it is from  7\n","4 evaluation points created.it is from  8\n","4 evaluation points created.it is from  9\n","2 evaluation points created.it is from  10\n","2 evaluation points created.it is from  11\n","2 evaluation points created.it is from  12\n","2 evaluation points created.it is from  13\n","2 evaluation points created.it is from  14\n","6 evaluation points created.it is from  15\n","6 evaluation points created.it is from  16\n","6 evaluation points created.it is from  17\n","6 evaluation points created.it is from  18\n","6 evaluation points created.it is from  19\n","this is random\n","6 evaluation points created.it is from  20\n","this is random\n","6 evaluation points created.it is from  21\n","6 evaluation points created.it is from  22\n","6 evaluation points created.it is from  23\n","6 evaluation points created.it is from  24\n","6 evaluation points created.it is from  25\n","6 evaluation points created.it is from  26\n","this is random\n","6 evaluation points created.it is from  27\n","6 evaluation points created.it is from  28\n","1 evaluation points created.it is from  29\n","1 evaluation points created.it is from  30\n","1 evaluation points created.it is from  31\n","4 evaluation points created.it is from  32\n","4 evaluation points created.it is from  33\n","this is random\n","4 evaluation points created.it is from  34\n","4 evaluation points created.it is from  35\n","4 evaluation points created.it is from  36\n","4 evaluation points created.it is from  37\n","4 evaluation points created.it is from  38\n","4 evaluation points created.it is from  39\n","this is random\n","4 evaluation points created.it is from  40\n","2 evaluation points created.it is from  41\n","2 evaluation points created.it is from  42\n","2 evaluation points created.it is from  43\n","2 evaluation points created.it is from  44\n","2 evaluation points created.it is from  45\n","2 evaluation points created.it is from  46\n","this is random\n","2 evaluation points created.it is from  47\n","2 evaluation points created.it is from  48\n","2 evaluation points created.it is from  49\n","this is random\n","2 evaluation points created.it is from  50\n","2 evaluation points created.it is from  51\n","2 evaluation points created.it is from  52\n","this is random\n","2 evaluation points created.it is from  53\n","this is random\n","3 evaluation points created.it is from  54\n","this is random\n","3 evaluation points created.it is from  55\n","3 evaluation points created.it is from  56\n","this is random\n","3 evaluation points created.it is from  57\n","3 evaluation points created.it is from  58\n","3 evaluation points created.it is from  59\n","this is random\n","3 evaluation points created.it is from  60\n","this is random\n","3 evaluation points created.it is from  61\n","this is random\n","3 evaluation points created.it is from  62\n","this is random\n","3 evaluation points created.it is from  63\n","3 evaluation points created.it is from  64\n","this is random\n","2 evaluation points created.it is from  65\n","2 evaluation points created.it is from  66\n","this is random\n","2 evaluation points created.it is from  67\n","this is random\n","2 evaluation points created.it is from  68\n","this is random\n","2 evaluation points created.it is from  69\n","4 evaluation points created.it is from  70\n","4 evaluation points created.it is from  71\n","4 evaluation points created.it is from  72\n","4 evaluation points created.it is from  73\n","this is random\n","4 evaluation points created.it is from  74\n","4 evaluation points created.it is from  75\n","this is random\n","1 evaluation points created.it is from  76\n","1 evaluation points created.it is from  77\n","1 evaluation points created.it is from  78\n","2 evaluation points created.it is from  79\n","2 evaluation points created.it is from  80\n","2 evaluation points created.it is from  81\n","2 evaluation points created.it is from  82\n","2 evaluation points created.it is from  83\n","2 evaluation points created.it is from  84\n","2 evaluation points created.it is from  85\n","this is random\n","11 evaluation points created.it is from  86\n","11 evaluation points created.it is from  87\n","11 evaluation points created.it is from  88\n","11 evaluation points created.it is from  89\n","11 evaluation points created.it is from  90\n","11 evaluation points created.it is from  91\n","11 evaluation points created.it is from  92\n","11 evaluation points created.it is from  93\n","11 evaluation points created.it is from  94\n","11 evaluation points created.it is from  95\n","11 evaluation points created.it is from  96\n","11 evaluation points created.it is from  97\n","11 evaluation points created.it is from  98\n","11 evaluation points created.it is from  99\n","11 evaluation points created.it is from  100\n","11 evaluation points created.it is from  101\n","11 evaluation points created.it is from  102\n","11 evaluation points created.it is from  103\n","11 evaluation points created.it is from  104\n","11 evaluation points created.it is from  105\n","11 evaluation points created.it is from  106\n","11 evaluation points created.it is from  107\n","11 evaluation points created.it is from  108\n","7 evaluation points created.it is from  109\n","7 evaluation points created.it is from  110\n","7 evaluation points created.it is from  111\n","7 evaluation points created.it is from  112\n","this is random\n","7 evaluation points created.it is from  113\n","this is random\n","7 evaluation points created.it is from  114\n","this is random\n","7 evaluation points created.it is from  115\n","7 evaluation points created.it is from  116\n","7 evaluation points created.it is from  117\n","this is random\n","7 evaluation points created.it is from  118\n","7 evaluation points created.it is from  119\n","this is random\n","7 evaluation points created.it is from  120\n","7 evaluation points created.it is from  121\n","2 evaluation points created.it is from  122\n","9 evaluation points created.it is from  123\n","this is random\n","9 evaluation points created.it is from  124\n","9 evaluation points created.it is from  125\n","this is random\n","9 evaluation points created.it is from  126\n","this is random\n","9 evaluation points created.it is from  127\n","this is random\n","9 evaluation points created.it is from  128\n","9 evaluation points created.it is from  129\n","this is random\n","9 evaluation points created.it is from  130\n","9 evaluation points created.it is from  131\n","this is random\n","9 evaluation points created.it is from  132\n","1 evaluation points created.it is from  133\n","1 evaluation points created.it is from  134\n","this is random\n","1 evaluation points created.it is from  135\n","1 evaluation points created.it is from  136\n","1 evaluation points created.it is from  137\n","1 evaluation points created.it is from  138\n","2 evaluation points created.it is from  139\n","2 evaluation points created.it is from  140\n","2 evaluation points created.it is from  141\n","2 evaluation points created.it is from  142\n","2 evaluation points created.it is from  143\n","2 evaluation points created.it is from  144\n","2 evaluation points created.it is from  145\n","this is random\n","2 evaluation points created.it is from  146\n","2 evaluation points created.it is from  147\n","2 evaluation points created.it is from  148\n","2 evaluation points created.it is from  149\n","2 evaluation points created.it is from  150\n","2 evaluation points created.it is from  151\n","2 evaluation points created.it is from  152\n","2 evaluation points created.it is from  153\n","this is random\n","2 evaluation points created.it is from  154\n","2 evaluation points created.it is from  155\n","2 evaluation points created.it is from  156\n","this is random\n","2 evaluation points created.it is from  157\n","this is random\n","2 evaluation points created.it is from  158\n","this is random\n","4 evaluation points created.it is from  159\n","5 evaluation points created.it is from  160\n","this is random\n","5 evaluation points created.it is from  161\n","this is random\n","5 evaluation points created.it is from  162\n","this is random\n","5 evaluation points created.it is from  163\n","5 evaluation points created.it is from  164\n","this is random\n","5 evaluation points created.it is from  165\n","this is random\n","5 evaluation points created.it is from  166\n","this is random\n","5 evaluation points created.it is from  167\n","this is random\n","5 evaluation points created.it is from  168\n","5 evaluation points created.it is from  169\n","2 evaluation points created.it is from  170\n","2 evaluation points created.it is from  171\n","2 evaluation points created.it is from  172\n","2 evaluation points created.it is from  173\n","2 evaluation points created.it is from  174\n","2 evaluation points created.it is from  175\n","2 evaluation points created.it is from  176\n","2 evaluation points created.it is from  177\n","2 evaluation points created.it is from  178\n","2 evaluation points created.it is from  179\n","2 evaluation points created.it is from  180\n","2 evaluation points created.it is from  181\n","2 evaluation points created.it is from  182\n","2 evaluation points created.it is from  183\n","2 evaluation points created.it is from  184\n","2 evaluation points created.it is from  185\n","1 evaluation points created.it is from  186\n","1 evaluation points created.it is from  187\n","1 evaluation points created.it is from  188\n","8 evaluation points created.it is from  189\n","8 evaluation points created.it is from  190\n","this is random\n","8 evaluation points created.it is from  191\n","this is random\n","8 evaluation points created.it is from  192\n","8 evaluation points created.it is from  193\n","this is random\n","8 evaluation points created.it is from  194\n","8 evaluation points created.it is from  195\n","this is random\n","8 evaluation points created.it is from  196\n","8 evaluation points created.it is from  197\n","this is random\n","8 evaluation points created.it is from  198\n","this is random\n","8 evaluation points created.it is from  199\n","this is random\n","8 evaluation points created.it is from  200\n","8 evaluation points created.it is from  201\n","this is random\n","8 evaluation points created.it is from  202\n","this is random\n","11 evaluation points created.it is from  203\n","11 evaluation points created.it is from  204\n","this is random\n","11 evaluation points created.it is from  205\n","11 evaluation points created.it is from  206\n","11 evaluation points created.it is from  207\n","11 evaluation points created.it is from  208\n","11 evaluation points created.it is from  209\n","11 evaluation points created.it is from  210\n","11 evaluation points created.it is from  211\n","11 evaluation points created.it is from  212\n","11 evaluation points created.it is from  213\n","11 evaluation points created.it is from  214\n","11 evaluation points created.it is from  215\n","11 evaluation points created.it is from  216\n","11 evaluation points created.it is from  217\n","11 evaluation points created.it is from  218\n","11 evaluation points created.it is from  219\n","11 evaluation points created.it is from  220\n","11 evaluation points created.it is from  221\n","11 evaluation points created.it is from  222\n","11 evaluation points created.it is from  223\n","11 evaluation points created.it is from  224\n","9 evaluation points created.it is from  225\n","this is random\n","9 evaluation points created.it is from  226\n","this is random\n","9 evaluation points created.it is from  227\n","this is random\n","9 evaluation points created.it is from  228\n","9 evaluation points created.it is from  229\n","9 evaluation points created.it is from  230\n","this is random\n","9 evaluation points created.it is from  231\n","9 evaluation points created.it is from  232\n","9 evaluation points created.it is from  233\n","this is random\n","9 evaluation points created.it is from  234\n","this is random\n","2 evaluation points created.it is from  235\n","2 evaluation points created.it is from  236\n","this is random\n","5 evaluation points created.it is from  237\n","5 evaluation points created.it is from  238\n","5 evaluation points created.it is from  239\n","5 evaluation points created.it is from  240\n","this is random\n","5 evaluation points created.it is from  241\n","this is random\n","5 evaluation points created.it is from  242\n","this is random\n","5 evaluation points created.it is from  243\n","5 evaluation points created.it is from  244\n","5 evaluation points created.it is from  245\n","5 evaluation points created.it is from  246\n","5 evaluation points created.it is from  247\n","this is random\n","5 evaluation points created.it is from  248\n","this is random\n","5 evaluation points created.it is from  249\n","5 evaluation points created.it is from  250\n","5 evaluation points created.it is from  251\n","5 evaluation points created.it is from  252\n","29 evaluation points created.it is from  253\n","this is random\n","29 evaluation points created.it is from  254\n","29 evaluation points created.it is from  255\n","29 evaluation points created.it is from  256\n","this is random\n","29 evaluation points created.it is from  257\n","29 evaluation points created.it is from  258\n","29 evaluation points created.it is from  259\n","29 evaluation points created.it is from  260\n","29 evaluation points created.it is from  261\n","29 evaluation points created.it is from  262\n","7 evaluation points created.it is from  263\n","this is random\n","7 evaluation points created.it is from  264\n","7 evaluation points created.it is from  265\n","7 evaluation points created.it is from  266\n","7 evaluation points created.it is from  267\n","7 evaluation points created.it is from  268\n","this is random\n","7 evaluation points created.it is from  269\n","this is random\n","7 evaluation points created.it is from  270\n","7 evaluation points created.it is from  271\n","7 evaluation points created.it is from  272\n","this is random\n","7 evaluation points created.it is from  273\n","this is random\n","7 evaluation points created.it is from  274\n","this is random\n","7 evaluation points created.it is from  275\n","7 evaluation points created.it is from  276\n","this is random\n","7 evaluation points created.it is from  277\n","this is random\n","7 evaluation points created.it is from  278\n","6 evaluation points created.it is from  279\n","6 evaluation points created.it is from  280\n","6 evaluation points created.it is from  281\n","6 evaluation points created.it is from  282\n","6 evaluation points created.it is from  283\n","6 evaluation points created.it is from  284\n","6 evaluation points created.it is from  285\n","this is random\n","6 evaluation points created.it is from  286\n","this is random\n","6 evaluation points created.it is from  287\n","6 evaluation points created.it is from  288\n","6 evaluation points created.it is from  289\n","this is random\n","6 evaluation points created.it is from  290\n","6 evaluation points created.it is from  291\n","this is random\n","6 evaluation points created.it is from  292\n","this is random\n","6 evaluation points created.it is from  293\n","this is random\n","6 evaluation points created.it is from  294\n","6 evaluation points created.it is from  295\n","6 evaluation points created.it is from  296\n","4 evaluation points created.it is from  297\n","this is random\n","4 evaluation points created.it is from  298\n","4 evaluation points created.it is from  299\n","4 evaluation points created.it is from  300\n","4 evaluation points created.it is from  301\n","4 evaluation points created.it is from  302\n","this is random\n","4 evaluation points created.it is from  303\n","this is random\n","4 evaluation points created.it is from  304\n","4 evaluation points created.it is from  305\n","4 evaluation points created.it is from  306\n","5 evaluation points created.it is from  307\n","5 evaluation points created.it is from  308\n","this is random\n","5 evaluation points created.it is from  309\n","this is random\n","5 evaluation points created.it is from  310\n","5 evaluation points created.it is from  311\n","5 evaluation points created.it is from  312\n","5 evaluation points created.it is from  313\n","5 evaluation points created.it is from  314\n","this is random\n","5 evaluation points created.it is from  315\n","5 evaluation points created.it is from  316\n","5 evaluation points created.it is from  317\n","5 evaluation points created.it is from  318\n","5 evaluation points created.it is from  319\n","this is random\n","5 evaluation points created.it is from  320\n","this is random\n","5 evaluation points created.it is from  321\n","5 evaluation points created.it is from  322\n","this is random\n","5 evaluation points created.it is from  323\n","5 evaluation points created.it is from  324\n","this is random\n","5 evaluation points created.it is from  325\n","this is random\n","5 evaluation points created.it is from  326\n","this is random\n","2 evaluation points created.it is from  327\n","2 evaluation points created.it is from  328\n","2 evaluation points created.it is from  329\n","this is random\n","2 evaluation points created.it is from  330\n","this is random\n","2 evaluation points created.it is from  331\n","2 evaluation points created.it is from  332\n","2 evaluation points created.it is from  333\n","9 evaluation points created.it is from  334\n","this is random\n","9 evaluation points created.it is from  335\n","this is random\n","9 evaluation points created.it is from  336\n","this is random\n","9 evaluation points created.it is from  337\n","9 evaluation points created.it is from  338\n","9 evaluation points created.it is from  339\n","9 evaluation points created.it is from  340\n","this is random\n","9 evaluation points created.it is from  341\n","9 evaluation points created.it is from  342\n","this is random\n","9 evaluation points created.it is from  343\n","this is random\n","9 evaluation points created.it is from  344\n","8 evaluation points created.it is from  345\n","8 evaluation points created.it is from  346\n","this is random\n","8 evaluation points created.it is from  347\n","8 evaluation points created.it is from  348\n","8 evaluation points created.it is from  349\n","this is random\n","8 evaluation points created.it is from  350\n","this is random\n","8 evaluation points created.it is from  351\n","this is random\n","8 evaluation points created.it is from  352\n","8 evaluation points created.it is from  353\n","8 evaluation points created.it is from  354\n","8 evaluation points created.it is from  355\n","8 evaluation points created.it is from  356\n","this is random\n","8 evaluation points created.it is from  357\n","this is random\n","8 evaluation points created.it is from  358\n","8 evaluation points created.it is from  359\n","8 evaluation points created.it is from  360\n","2 evaluation points created.it is from  361\n","2 evaluation points created.it is from  362\n","2 evaluation points created.it is from  363\n","this is random\n","2 evaluation points created.it is from  364\n","2 evaluation points created.it is from  365\n","3 evaluation points created.it is from  366\n","3 evaluation points created.it is from  367\n","this is random\n","3 evaluation points created.it is from  368\n","this is random\n","3 evaluation points created.it is from  369\n","3 evaluation points created.it is from  370\n","3 evaluation points created.it is from  371\n","3 evaluation points created.it is from  372\n","this is random\n","3 evaluation points created.it is from  373\n","3 evaluation points created.it is from  374\n","3 evaluation points created.it is from  375\n","3 evaluation points created.it is from  376\n","3 evaluation points created.it is from  377\n","3 evaluation points created.it is from  378\n","this is random\n","5 evaluation points created.it is from  379\n","5 evaluation points created.it is from  380\n","5 evaluation points created.it is from  381\n","this is random\n","5 evaluation points created.it is from  382\n","this is random\n","5 evaluation points created.it is from  383\n","this is random\n","5 evaluation points created.it is from  384\n","5 evaluation points created.it is from  385\n","5 evaluation points created.it is from  386\n","5 evaluation points created.it is from  387\n","5 evaluation points created.it is from  388\n","this is random\n","5 evaluation points created.it is from  389\n","5 evaluation points created.it is from  390\n","5 evaluation points created.it is from  391\n","5 evaluation points created.it is from  392\n","5 evaluation points created.it is from  393\n","5 evaluation points created.it is from  394\n","5 evaluation points created.it is from  395\n","this is random\n","12 evaluation points created.it is from  396\n","12 evaluation points created.it is from  397\n","this is random\n","12 evaluation points created.it is from  398\n","12 evaluation points created.it is from  399\n","12 evaluation points created.it is from  400\n","12 evaluation points created.it is from  401\n","12 evaluation points created.it is from  402\n","12 evaluation points created.it is from  403\n","this is random\n","12 evaluation points created.it is from  404\n","12 evaluation points created.it is from  405\n","2 evaluation points created.it is from  406\n","2 evaluation points created.it is from  407\n","2 evaluation points created.it is from  408\n","2 evaluation points created.it is from  409\n","2 evaluation points created.it is from  410\n","2 evaluation points created.it is from  411\n","2 evaluation points created.it is from  412\n","this is random\n","10 evaluation points created.it is from  413\n","10 evaluation points created.it is from  414\n","10 evaluation points created.it is from  415\n","10 evaluation points created.it is from  416\n","10 evaluation points created.it is from  417\n","10 evaluation points created.it is from  418\n","10 evaluation points created.it is from  419\n","this is random\n","10 evaluation points created.it is from  420\n","10 evaluation points created.it is from  421\n","10 evaluation points created.it is from  422\n","this is random\n","10 evaluation points created.it is from  423\n","10 evaluation points created.it is from  424\n","9 evaluation points created.it is from  425\n","this is random\n","9 evaluation points created.it is from  426\n","this is random\n","9 evaluation points created.it is from  427\n","9 evaluation points created.it is from  428\n","9 evaluation points created.it is from  429\n","9 evaluation points created.it is from  430\n","9 evaluation points created.it is from  431\n","9 evaluation points created.it is from  432\n","9 evaluation points created.it is from  433\n","this is random\n","9 evaluation points created.it is from  434\n","this is random\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e2Gi1xi6qRIK","colab":{"base_uri":"https://localhost:8080/","height":274},"executionInfo":{"status":"error","timestamp":1613368813010,"user_tz":-540,"elapsed":858,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"721cd650-f5d2-42ae-9188-56dee8087425"},"source":["# load test_input.txt and make test_result.txt file\n","import numpy as np\n","\n","with open('/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src/question_ex.json') as f:\n","    question_ex = json.load(f)\n","\n","\n","eval_data = []\n","path = '/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/'\n","with open(path+'data/test/test_input.txt') as f:\n","    for i in f:\n","        temp = i.split('\\t')\n","        context = temp[0].strip()\n","        question = temp[1].strip()\n","        make_data = {'paragraphs':[{'qas':[{'answers':[{'text':None,'answer_start': None,'id':'for inference'}],'question': question}],'context': context}]}\n","        eval_data.append(make_data)\n","\n","eval_result = {}\n","eval_result['version'] = 'for_inference'\n","eval_result['data'] = eval_data\n","\n","\n","class SquadExample2:\n","    def __init__(self, question, context, max_len=511):\n","        self.question = question\n","        self.context = context\n","        self.skip = False\n","        self.max_len = max_len\n","\n","    def preprocess(self):\n","        context = self.context\n","        question = self.question\n","\n","        # Clean context, answer and question\n","        context = \" \".join(str(context).split())\n","        question = \" \".join(str(question).split())\n","\n","        # Tokenize context\n","        tokenized_context = tokenizer.encode(context)\n","\n","        # Tokenize question\n","        tokenized_question = tokenizer.encode(question)\n","\n","        # Create inputs\n","        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n","        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n","            tokenized_question.ids[1:]\n","        )\n","        attention_mask = [1] * len(input_ids)\n","\n","        # Pad and create attention masks.\n","        # Skip if truncation is needed\n","        padding_length = self.max_len - len(input_ids)\n","        if padding_length > 0:  # pad\n","            input_ids = input_ids + ([0] * padding_length)\n","            attention_mask = attention_mask + ([0] * padding_length)\n","            token_type_ids = token_type_ids + ([0] * padding_length)\n","        elif padding_length < 0:  # skip\n","            self.skip = True\n","            return\n","        \n","        #error handling\n","        # Truncate the comments\n","        # if len(input_ids) >  510:\n","        #     input_ids = input_ids[0:128] + input_ids[-382:-1] #510\n","        #     attention_mask = attention_mask[0:128] + attention_mask[-382:-1]\n","        #     token_type_ids = token_type_ids[0:128] + token_type_ids[-382:-1]\n","        ##### \n","        \n","        self.input_ids = input_ids\n","        self.token_type_ids = token_type_ids\n","        self.attention_mask = attention_mask\n","        self.context_token_to_char = tokenized_context.offsets\n","\n","\n","def create_squad_examples2(raw_data):\n","    squad_examples = []\n","    for item in evres:\n","        context = item[\"context\"][0]\n","        question = item[\"question\"]     \n","        squad_eg = SquadExample2(question, context)\n","        squad_eg.preprocess()\n","        squad_examples.append(squad_eg)\n","    return squad_examples\n","\n","\n","def create_inputs_targets2(squad_examples):\n","    dataset_dict = {\n","        \"input_ids\": [],\n","        \"token_type_ids\": [],\n","        \"attention_mask\": []}\n","    \n","    for item in squad_examples:\n","        #item = suqand_example class\n","        if item.skip == False:\n","            for key in dataset_dict:\n","                dataset_dict[key].append(getattr(item, key)) #squad_example.input_ids \n","    for key in dataset_dict:\n","        dataset_dict[key] = np.array(dataset_dict[key])\n","\n","    x = [\n","        dataset_dict[\"input_ids\"],\n","        dataset_dict[\"token_type_ids\"],\n","        dataset_dict[\"attention_mask\"],\n","    ]\n","    \n","    return x\n","\n","max_len = 511\n","inferences = {}\n","for n in range(len(eval_result['data'])):\n","    evres = []\n","    question = eval_result['data'][n]['paragraphs'][0]['qas'][0]['question']\n","    eval_splits = eval_result['data'][n]['paragraphs'][0]['context'].split('.')\n","    \n","    total_txt = []\n","    temp_txt = []\n","    permit = 490\n","\n","    for idx in range(len(eval_splits)):\n","        permit -= len(eval_splits[idx])\n","        if permit < 0:\n","            permit = 490\n","            total_txt.append([\".\".join(temp_txt) + '.'])\n","            temp_txt = []\n","        \n","        temp_txt.append(eval_splits[idx])\n","        if idx == (len(eval_splits) - 1):\n","            total_txt.append([\".\".join(temp_txt)])\n","    evdict = {}\n","\n","    for ids,str_txt in enumerate(total_txt):\n","\n","        evdict['question'] = question\n","        evdict['context'] = str_txt\n","\n","        evres.append(evdict)\n","        evdict = {}\n","\n","    evres_squad = create_squad_examples2(evres)\n","    x_eval = create_inputs_targets2(evres_squad)\n","    print(f\"{len(evres_squad)} evaluation points created.it is from \",n)\n","\n","    pred_start, pred_end = loaded_model.predict(x_eval)\n","\n","    eval_examples_no_skip = [_ for _ in evres_squad if _.skip == False] # evres_squad or eval_squad_examples\n","    inference = []\n","    status = 0\n","    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n","        squad_eg = eval_examples_no_skip[idx]\n","        offsets = squad_eg.context_token_to_char\n","\n","        start = np.argmax(start)\n","\n","        end = np.argmax(end)\n","        if start >= len(offsets):\n","            continue\n","        pred_char_start = offsets[start][0]\n","        if end < len(offsets):\n","            pred_char_end = offsets[end][1]\n","            pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n","        else:\n","            pred_ans = squad_eg.context[pred_char_start:]\n","        normalized_pred_ans = normalized_answer(pred_ans)\n","        for qs_ex in question_ex[question]:\n","            if normalized_answer(qs_ex.strip()) != '':\n","                if (normalized_answer(qs_ex.strip()) in normalized_pred_ans) or (normalized_pred_ans in normalized_answer(qs_ex.strip())):\n","                    if normalized_pred_ans != '':\n","                        inference.append(normalized_pred_ans)\n","                        status = 1\n","                else:\n","                    if (normalized_pred_ans[:int(len(normalized_pred_ans) / 3)] in normalized_answer(qs_ex.strip())) or (normalized_pred_ans[1 * int(len(normalized_pred_ans) / 4):] in normalized_answer(qs_ex.strip())):\n","                        if normalized_pred_ans != '':\n","                            inference.append(normalized_pred_ans)\n","                            status = 1\n","        inference = np.unique(inference).tolist()\n","    if status == 1:\n","        if (inference != ['']):\n","            inferences[n] = inference\n","    else:\n","        temp_li = []\n","        for qex in question_ex[question]:\n","            if normalized_pred_ans[:5] in qex:\n","                temp_li.append(qex)\n","        len_max = len(temp_li)\n","        if len_max == 0:\n","            inferences[n] = [normalized_answer(question_ex[question][np.random.randint(0,len(question_ex[question]))])]\n","        else:\n","            inferences[n] = [normalized_answer(temp_li[np.random.randint(0,len_max)])]\n","#inferences\n","ban_list = ['a 환자분 입원하러 오셨나요 b 네','a 입원하러 오셨나요 b 네 오후 4시에 병동으로 올라오라고 했어요','a 네 되셨습니다','a 안녕하세요 혹시 오늘 어떤 것 때문에 병원에 오시게 되셨을까요 b 아 네 안녕하세요 다름이 아니라 대변에서 피가나오는 증상이랑 설사 증상이 있어서 방문하게 되었습니다 b 일주일 전쯤에 상한 음식을 먹고 계속 그러네요','a 안녕하세요 혹시 오늘 어떤 것 때문에 병원에 오시게 되셨을까요 b 숨쉬기가 힘들어요 a 언제부터 그러셨나요 b 몇 일 전부터 기침이 계속 나오더니 삼십분전부터 숨쉬기가 힘들어요','a 안녕하세요 혹시 오늘 어떤 것 때문에 병원에 오시게 되셨을까요 b 숨쉬기가 힘들어요','a 기형이 있으신가요 b 아니요 없습니다','a 흡연을 하시나요 b 네 그렇습니다','a 발진이 있으신가요 b 아니요 없습니다','a 흡연을 하시나요 b 네 그렇습니다 a 흡연량이 얼마나 되나요 b 일주일에 한 갑 피웁니다 a 흡연기간이 얼마나 되시나요 b 30년 정도 되었습니다 a 음주를 하시나요 b 네 그렇습니다 a 음주량과 횟수 종류가 어떻게 되시나요 b 일주일에 23번 마시고 한 번에 소주 2병 정도 마십니다 a 음주기간이 얼마나 되시나요 b 30년 조금 넘었습니다 a 어디를 통해서 입원을 하셨나요 b 이 병원 외래를 방문 후 입원하게 되었습니다','a 최근 삼 개월 이내에 입원치료를 받으신 적이 있나요 b 아니요 없습니다 a 흡연을 하시나요 b 네 그렇습니다 a 흡연량이 얼마나 되나요 b 일주일에 한 갑 피웁니다 a 흡연기간이 얼마나 되시나요 b 30년 정도 되었습니다 a 음주를 하시나요 b 네 그렇습니다 a 음주량과 횟수 종류가 어떻게 되시나요 b 일주일에 23번 마시고 한 번에 소주 2병 정도 마십니다 a 음주기간이 얼마나 되시나요 b 30년 조금 넘었습니다 a 어디를 통해서 입원을 하셨나요 b 이 병원 외래를 방문 후 입원하게 되었습니다','a 음식이나 약에 알레르기 반응이 있으신가요 b 아니요 없습니다','a 수술이나 입원치료를 받으신 적이 있으신가요 b 아니요 없습니다','a 수혈을 하신 적이 있으신가요 b 아니요 없습니다','a 현재 통증이 있으신가요 b 네 있습니다','a 최근 삼 개월 이내에 입원치료를 받으신 적이 있나요 b 아니요 없습니다 a 흡연을 하시나요 b 네 그렇습니다 a 흡연량이 얼마나 되나요 b 일주일에 한 갑 피웁니다 a 흡연기간이 얼마나 되시나요 b 30년 정도 되었습니다 a 음주를 하시나요 b 네 그렇습니다 a 음주량과 횟수 종류가 어떻게 되시나요 b 일주일에 23번 마시고 한 번에 소주 2병 정도 마십니다 a 음주기간이 얼마나 되시나요 b 30년 조금 넘었습니다 a 어디를 통해서 입원을 하셨나요 b 이 병원 외래를 방문 후 입원하게 되었습니다']\n","for i in inferences.keys():\n","    if len(inferences[i]) > 1:\n","        not_ban = []\n","        for inf in inferences[i]:\n","            if inf not in ban_list:\n","                not_ban.append(inf)\n","        inferences[i] = not_ban\n","        \n","        if len(inferences[i]) > 1:\n","            inferences[i] = [inferences[i][0]]\n","            \n","with open('/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/result/test_result.txt','w',encoding='utf-8') as f:\n","    for ke in inferences.keys():\n","        if inferences[ke] == []:\n","            f.write('A: ~ 인가요? B: 네. 아니요')\n","            f.write('\\n')\n","        else:\n","            f.write(inferences[ke][0])\n","            f.write('\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4 evaluation points created.it is from  0\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-091606d815ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mx_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_inputs_targets2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevres_squad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{len(evres_squad)} evaluation points created.it is from \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"답 : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalized_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_eval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraphs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"question: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mpred_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'raw_eval_data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"bvXCfiNz97s-"},"source":[""],"execution_count":null,"outputs":[]}]}