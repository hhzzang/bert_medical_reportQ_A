{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"model_train.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PAhdaTVX7d24","executionInfo":{"status":"ok","timestamp":1612809388974,"user_tz":-540,"elapsed":15845,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"dce10836-8f12-4bee-eb45-bf7b528e5e49"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezCfUuIx7d28","executionInfo":{"status":"ok","timestamp":1612809430261,"user_tz":-540,"elapsed":3137,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"0565a04e-62be-45e7-e7a4-140cd64dc6b9"},"source":["!pip install tensorflow-addons"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.8.3)\n","Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (2.7.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"It4YN-977d28","executionInfo":{"status":"ok","timestamp":1612809433715,"user_tz":-540,"elapsed":5091,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"79287fe0-3960-426c-9709-b339bac1b866"},"source":["!pip install tokenizers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tokenizers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 8.5MB/s \n","\u001b[?25hInstalling collected packages: tokenizers\n","Successfully installed tokenizers-0.10.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLMvv89i7d28","executionInfo":{"status":"ok","timestamp":1612809439779,"user_tz":-540,"elapsed":10193,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"8e9a31d9-c38b-4207-d325-8efaa9ab46d3"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/bb/61bf4a221150aff2756935d032a83f2e68976b050e8606d990a0f3054179/transformers-4.3.0-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 8.3MB/s \n","\u001b[?25hRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 40.7MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=8b876e836bf365bb5e1d474a583dc81f2590a9d7dc5a97ac3856e9475a93212a\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 transformers-4.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fo2ObVOX7d28","executionInfo":{"status":"ok","timestamp":1612809420722,"user_tz":-540,"elapsed":3192,"user":{"displayName":"Hyukhun Koh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFmQFILEivwghVAUWhEtg_t_piycJmb4VOpPQS=s64","userId":"04457846064668801006"}},"outputId":"17d4081c-7681-49f1-c055-6cdd39464012"},"source":["!pip install wget"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KNmaUL5v7d29"},"source":["import os\n","import re\n","import json\n","import string\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tokenizers import BertWordPieceTokenizer\n","from transformers import BertTokenizer, TFBertModel\n","\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import matplotlib.pyplot as plt\n","import urllib\n","import wget\n","\n","MAX_LEN = 511\n","EPOCHS = 3\n","VERBOSE = 2\n","BATCH_SIZE = 8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKBtD8ty7d29"},"source":["DATA_OUT_PATH = '/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-hKdpan7d29"},"source":["def plot_graphs(history, string, string_1, string_2):\n","    # loss \n","    plt.plot(history.history[string])\n","    plt.plot(history.history[string_1])\n","    plt.plot(history.history[string_2])\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(string)\n","    plt.legend([string, string_1, string_2])\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyWFS6w-7d29"},"source":["SEED_NUM = 1234\n","tf.random.set_seed(SEED_NUM)\n","np.random.seed(SEED_NUM)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWG-kTCa7d2-"},"source":["# Save the slow pretrained tokenizer\n","slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", lowercase=False)\n","save_path = \"bert-base-multilingual-cased/\"\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","slow_tokenizer.save_pretrained(save_path)\n","\n","# Load the fast tokenizer from saved file\n","tokenizer = BertWordPieceTokenizer(\"bert-base-multilingual-cased/vocab.txt\", lowercase=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oKJtqgu77d2-"},"source":["class Squad:\n","    def __init__(self, question, context, start_char_idx, answer_text):\n","        self.question = question\n","        self.context = context\n","        self.start_char_idx = start_char_idx\n","        self.answer_text = answer_text\n","        self.skip = False\n","\n","    def preprocess(self):\n","        context = self.context\n","        question = self.question\n","        answer_text = self.answer_text\n","        start_char_idx = self.start_char_idx\n","\n","        # Clean context, answer and question\n","        context = \" \".join(str(context).split())\n","        question = \" \".join(str(question).split())\n","        answer = \" \".join(str(answer_text).split())\n","\n","        # Find end character index of answer in context\n","        end_char_idx = start_char_idx + len(answer)\n","        if end_char_idx >= len(context):\n","            self.skip = True\n","            return\n","\n","        # Mark the character indexes in context that are in answer\n","        is_char_in_ans = [0] * len(context)\n","        for idx in range(start_char_idx, end_char_idx):\n","            is_char_in_ans[idx] = 1\n","\n","        # Tokenize context\n","        tokenized_context = tokenizer.encode(context)\n","\n","        # Find tokens that were created from answer characters\n","        ans_token_idx = []\n","        for idx, (start, end) in enumerate(tokenized_context.offsets):\n","            if sum(is_char_in_ans[start:end]) > 0:\n","                ans_token_idx.append(idx)\n","\n","        if len(ans_token_idx) == 0:\n","            self.skip = True\n","            return\n","\n","        # Find start and end token index for tokens from answer\n","        start_token_idx = ans_token_idx[0]\n","        end_token_idx = ans_token_idx[-1]\n","\n","        # Tokenize question\n","        tokenized_question = tokenizer.encode(question)\n","\n","        # Create inputs\n","        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n","        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(\n","            tokenized_question.ids[1:]\n","        )\n","        attention_mask = [1] * len(input_ids)\n","\n","        # Pad and create attention masks.\n","        # Skip if truncation is needed\n","        padding_length = MAX_LEN - len(input_ids)\n","        if padding_length > 0:  # pad\n","            input_ids = input_ids + ([0] * padding_length)\n","            attention_mask = attention_mask + ([0] * padding_length)\n","            token_type_ids = token_type_ids + ([0] * padding_length)\n","        elif padding_length < 0:  # skip\n","            self.skip = True\n","            return\n","        \n","        #함정카드\n","        # Truncate the comments\n","        if len(input_ids) >  512:\n","            input_ids = input_ids[0:128] + input_ids[-382:-1]\n","            attention_mask = attention_mask[0:128] + attention_mask[-382:-1]\n","            token_type_ids = token_type_ids[0:128] + token_type_ids[-382:-1]\n","        #####\n","        \n","        self.input_ids = input_ids\n","        self.token_type_ids = token_type_ids\n","        self.attention_mask = attention_mask\n","        self.start_token_idx = start_token_idx\n","        self.end_token_idx = end_token_idx\n","        self.context_token_to_char = tokenized_context.offsets\n","\n","\n","def create_squad_examples(raw_data):\n","    squad_examples = []\n","    for item in raw_data[\"data\"]:\n","        for para in item[\"paragraphs\"]:\n","            context = para[\"context\"]\n","            for qa in para[\"qas\"]:\n","                question = qa[\"question\"]\n","                answer_text = qa[\"answers\"][0][\"text\"]\n","                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n","                squad_eg = Squad(\n","                    question, context, start_char_idx, answer_text\n","                )\n","                squad_eg.preprocess()\n","                squad_examples.append(squad_eg)\n","    return squad_examples\n","\n","\n","def create_inputs_targets(squad_examples):\n","    dataset_dict = {\n","        \"input_ids\": [],\n","        \"token_type_ids\": [],\n","        \"attention_mask\": [],\n","        \"start_token_idx\": [],\n","        \"end_token_idx\": [],\n","    }\n","    for item in squad_examples:\n","        #item = suqand_example class\n","        if item.skip == False:\n","            for key in dataset_dict:\n","                dataset_dict[key].append(getattr(item, key))\n","    for key in dataset_dict:\n","        dataset_dict[key] = np.array(dataset_dict[key])\n","\n","    x = [\n","        dataset_dict[\"input_ids\"],\n","        dataset_dict[\"token_type_ids\"],\n","        dataset_dict[\"attention_mask\"],\n","    ]\n","    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n","    return x, y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbibrVPq7d2_"},"source":["#Data input\n","path = '/content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/src/'\n","with open(path+'extract_train_data.json') as f:\n","    raw_train_data = json.load(f)\n","\n","with open(path+'extract_dev_data.json') as f:\n","    raw_eval_data = json.load(f)\n","\n","\n","train_squad_examples = create_squad_examples(raw_train_data)\n","x_train, y_train = create_inputs_targets(train_squad_examples)\n","print(f\"{len(train_squad_examples)} training points created.\")\n","\n","eval_squad_examples = create_squad_examples(raw_eval_data)\n","x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n","print(f\"{len(eval_squad_examples)} evaluation points created.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMseJNnl7d2_"},"source":["class TFBERTQuestionAnswering(tf.keras.Model):\n","    def __init__(self, model_name, dir_path, num_class):\n","        super(TFBERTQuestionAnswering, self).__init__()\n","        \n","        self.encoder = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n","        self.start_logit = tf.keras.layers.Dense(num_class, name=\"start_logit\", use_bias=False)\n","        self.end_logit = tf.keras.layers.Dense(num_class, name=\"end_logit\", use_bias=False)\n","        self.flatten = tf.keras.layers.Flatten() \n","        self.softmax = tf.keras.layers.Activation(tf.keras.activations.softmax)\n","        \n","    def call(self, inputs):\n","        input_ids, token_type_ids, attention_mask = inputs\n","        embedding = self.encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n","        start_logits = self.start_logit(embedding)\n","        start_logits = self.flatten(start_logits)\n","        \n","        end_logits = self.end_logit(embedding)\n","        end_logits = self.flatten(end_logits)\n","        \n","        start_probs = self.softmax(start_logits)\n","        end_probs = self.softmax(end_logits)\n","    \n","        return start_probs, end_probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwNengcL7d3A"},"source":["import tensorflow_addons as tfa\n","korquad_model = TFBERTQuestionAnswering(model_name='./bert-base-multilingual-cased/',dir_path='bert_ckpt', num_class=1)\n","optimizer = tfa.optimizers.RectifiedAdam(lr=1e-5) #tf.keras.optimizers.RAdam(learning_rate=5e-5)\n","loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cvmKJSSP7d3A"},"source":["import re\n","import string\n","from collections import Counter\n","def normalize_answer(s):    \n","    def remove_(text):\n","        ''' 불필요한 기호 제거 '''\n","        text = re.sub(\"'\", \" \", text)\n","        text = re.sub('\"', \" \", text)\n","        text = re.sub('《', \" \", text)\n","        text = re.sub('》', \" \", text)\n","        text = re.sub('<', \" \", text)\n","        text = re.sub('>', \" \", text) \n","        text = re.sub('〈', \" \", text)\n","        text = re.sub('〉', \" \", text)   \n","        text = re.sub(\"\\(\", \" \", text)\n","        text = re.sub(\"\\)\", \" \", text)\n","        text = re.sub(\"‘\", \" \", text)\n","        text = re.sub(\"’\", \" \", text)      \n","        return text\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_punc(lower(remove_(s))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCwzm5ly7d3A"},"source":["exact_match_callback = ExactMatch(x_eval, y_eval)\n","korquad_model.compile(optimizer=optimizer, loss=[loss, loss])\n","history = korquad_model.fit(\n","    x_train,\n","    y_train,\n","    epochs=6,  # For demonstration, 3 epochs are recommended\n","    verbose=VERBOSE,\n","    batch_size=BATCH_SIZE,\n","    callbacks=[exact_match_callback] \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJtk3hOO7d3A"},"source":["print(history.history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DhMqpkn-7d3B"},"source":["%cd /content/gdrive/Shareddrives/슈퍼학부생 CREW/hackathon_2-2/\n","\n","!mkdir -p src\n","korquad_model.save_weights('src/6_epochs_extract_radam_weights.h5')"],"execution_count":null,"outputs":[]}]}